{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/CLIP-Driven-Universal-Model/universal/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word embedding\n",
      "train len 1860\n",
      "val len 285\n",
      "test len 508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 226\u001b[0m\n\u001b[1;32m    223\u001b[0m         validation(model, val_loader, args)\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# validation(model, val_loader, args)\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[43mvalid_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/clip/ClipReady/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPAOT_TEMP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m01 04 05 07 08 09 10_03 10_06 10_07 10_08 10_09 10_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 223\u001b[0m, in \u001b[0;36mvalid_loop\u001b[0;34m(data_root_path, dataset_list, datasetkey, num_workers, num_samples, cache_dataset, cache_rate, batch_size, roi, space, device)\u001b[0m\n\u001b[1;32m    221\u001b[0m     train(args, val_loader, model, optimizer, loss_seg_DICE, loss_seg_CE)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, ValLoader, args)\u001b[0m\n\u001b[1;32m    109\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiases\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    110\u001b[0m     individual_data[key] \u001b[38;5;241m=\u001b[39m {k:[[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_CLASS)] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mValLoader\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# print('%d processd' % (index))\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     image, label, name \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_label\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name, image\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/CLIP-Driven-Universal-Model/universal/lib/python3.8/site-packages/tqdm/std.py:671\u001b[0m, in \u001b[0;36mtqdm.__new__\u001b[0;34m(cls, *_, **__)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor_interval \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    669\u001b[0m                              \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor\u001b[38;5;241m.\u001b[39mreport()):\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 671\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor \u001b[38;5;241m=\u001b[39m \u001b[43mTMonitor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitor_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m    673\u001b[0m         warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm:disabling monitor support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    674\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (monitor_interval = 0) due to:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    675\u001b[0m              TqdmMonitorWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/CLIP-Driven-Universal-Model/universal/lib/python3.8/site-packages/tqdm/_monitor.py:39\u001b[0m, in \u001b[0;36mTMonitor.__init__\u001b[0;34m(self, tqdm_cls, sleep_interval)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_killed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m, Event)()\n\u001b[1;32m     38\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:849\u001b[0m, in \u001b[0;36mThread.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads can only be started once\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _active_limbo_lock:\n\u001b[1;32m    850\u001b[0m     _limbo[\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model_path\n",
    "import os\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from monai.inferers import sliding_window_inference\n",
    "os.chdir('/CLIP-Driven-Universal-Model')\n",
    "if '/CLIP-Driven-Universal-Model' not in sys.path:\n",
    "    sys.path.append('/CLIP-Driven-Universal-Model')\n",
    "from model.Universal_model import Universal_model\n",
    "from dataset.dataloader import get_loader\n",
    "from optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from utils.utils import get_key, dice_score, TEMPLATE, ORGAN_NAME\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"output/unet/epoch_50.pth\"\n",
    "# CUDA_VISIBLE_DEVICES=0 python -W ignore -m torch.distributed.launch --nproc_per_node=1 --master_port=1234 train.py --dist True --data_root_path /data/clip/ClipReady/ --num_workers 10 --num_samples 4 --cache_dataset --cache_rate 0.3 --uniform_sample --dataset_list PAOT_TEMP --datasetkey 01 04 05 07 08 09 10_03 10_06 10_07 10_08 10_09 10_10\n",
    "back_bone = log_name = 'unet'\n",
    "roi_x = roi_y = roi_z = 96\n",
    "trans_encoding = \"word_embedding\"\n",
    "word_embedding = \"./pretrained_weights/txt_encoding.pth\"\n",
    "from utils import loss\n",
    "NUM_CLASS = 32\n",
    "\n",
    "import os\n",
    "import torch\n",
    "def wrap_around(tensor, shift):\n",
    "    n = tensor.shape[0]\n",
    "    idx = [(i + shift) % n for i in range(n)]\n",
    "    return tensor[idx]\n",
    "def create_model(backbone, num_classes, trans_encoding, epoch, log_name, result_path=\"/data/result\", word_embedding = \"./pretrained_weights/txt_encoding.pth\" ,model_pretrain=None, model_checkpoint= None, roi_x=96, roi_y=96, roi_z=96, swap_word_embedding=0,device = 0):\n",
    "    # Construct the default model path if not provided\n",
    "    if model_checkpoint is None:\n",
    "        model_checkpoint = os.path.join(result_path, log_name, f\"epoch_{epoch}.pth\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Universal_model(\n",
    "        img_size=(roi_x, roi_y, roi_z),\n",
    "        in_channels=1,\n",
    "        out_channels=num_classes,\n",
    "        backbone=backbone,\n",
    "        encoding=trans_encoding\n",
    "    )\n",
    "\n",
    "    # Load pre-trained weights if provided\n",
    "    if model_pretrain is not None:\n",
    "        if model_pretrain is not None and os.path.exists(model_pretrain):\n",
    "            model.load_params(torch.load(model_pretrain)[\"state_dict\"])\n",
    "            print(f\"Loaded pre-trained model from {model_pretrain}\")\n",
    "        else:\n",
    "            print(f\"No pre-trained model found at {model_pretrain}, initializing model from scratch\")\n",
    "\n",
    "    # Load word embedding if encoding is 'word_embedding'\n",
    "    if trans_encoding == 'word_embedding':\n",
    "        word_embedding = torch.load(word_embedding)\n",
    "        if swap_word_embedding != -1:\n",
    "            word_embedding = wrap_around(word_embedding, swap_word_embedding)\n",
    "        model.organ_embedding.data = word_embedding.float()\n",
    "        print('Loaded word embedding')\n",
    "\n",
    "    # Log the model creation\n",
    "    log_dir = os.path.join(result_path, log_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file_path = os.path.join(log_dir, \"model_creation_log.txt\")\n",
    "\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(f\"Model created with parameters: backbone={backbone}, num_classes={num_classes}, trans_encoding={trans_encoding}, epoch={epoch}, roi=({roi_x}, {roi_y}, {roi_z})\\n\")\n",
    "    model.to(f\"cuda:{device}\")\n",
    "    return model\n",
    "\n",
    "# model = create_model(back_bone,NUM_CLASS, trans_encoding, 50, log_name, model_checkpoint=model_checkpoint)\n",
    "\n",
    "def train(args, train_loader, model, optimizer, loss_seg_DICE, loss_seg_CE):\n",
    "    model.train()\n",
    "    # change to validate mode\n",
    "    \n",
    "    loss_bce_ave = 0\n",
    "    loss_dice_ave = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        x, y, name = batch[\"image\"].to(args.device), batch[\"post_label\"].float().to(args.device), batch['name']\n",
    "        logit_map = model(x)\n",
    "\n",
    "        term_seg_Dice = loss_seg_DICE.forward(logit_map, y, name, TEMPLATE)\n",
    "        term_seg_BCE = loss_seg_CE.forward(logit_map, y, name, TEMPLATE)\n",
    "        loss = term_seg_BCE + term_seg_Dice\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(\n",
    "            \"Epoch=%d: Training (%d / %d Steps) (dice_loss=%2.5f, bce_loss=%2.5f)\" % (\n",
    "                args.epoch, step, len(train_loader), term_seg_Dice.item(), term_seg_BCE.item())\n",
    "        )\n",
    "        loss_bce_ave += term_seg_BCE.item()\n",
    "        loss_dice_ave += term_seg_Dice.item()\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Epoch=%d: ave_dice_loss=%2.5f, ave_bce_loss=%2.5f' % (args.epoch, loss_dice_ave/len(epoch_iterator), loss_bce_ave/len(epoch_iterator)))\n",
    "    \n",
    "    return loss_dice_ave/len(epoch_iterator), loss_bce_ave/len(epoch_iterator)\n",
    "\n",
    "def validation(model, ValLoader, args):\n",
    "    model.eval()\n",
    "    dice_list = {}\n",
    "    individual_data = {}\n",
    "    for key in TEMPLATE.keys():\n",
    "        dice_list[key] = np.zeros((2, NUM_CLASS)) # 1st row for dice, 2nd row for count\n",
    "        keys = [\"preds\", \"labels\", \"weights\", \"biases\",\"x_feature\"]\n",
    "        individual_data[key] = {k:[[] for _ in range(NUM_CLASS)] for k in keys}\n",
    "\n",
    "    for index, batch in enumerate(tqdm(ValLoader)):\n",
    "        # print('%d processd' % (index))\n",
    "        image, label, name = batch[\"image\"].to(args.device), batch[\"post_label\"], batch[\"name\"]\n",
    "        print(name, image.shape)\n",
    "        with torch.no_grad():\n",
    "            pred, list_list_x_feat, list_list_xs, list_list_weights,list_list_biases = sliding_window_inference_with_meta_data(image, (args.roi_x, args.roi_y, args.roi_z), 1, model)\n",
    "            pred_sigmoid = F.sigmoid(pred)\n",
    "        \n",
    "        B = pred_sigmoid.shape[0]\n",
    "        for b in range(B):\n",
    "            template_key = get_key(name[b])\n",
    "            organ_list = TEMPLATE[template_key]\n",
    "            for organ in organ_list:\n",
    "                dice_organ = dice_score(pred_sigmoid[b,organ-1,:,:,:], label[b,organ-1,:,:,:].cuda())\n",
    "                dice_list[template_key][0][organ-1] += dice_organ[0]\n",
    "                dice_list[template_key][1][organ-1] += 1\n",
    "                \n",
    "                individual_data[key][\"preds\"][organ-1].append(pred_sigmoid[b,organ-1,:,:,:])\n",
    "                individual_data[key][\"labels\"][organ-1].append(label[b,organ-1,:,:,:])\n",
    "                individual_data[key][\"x_feature\"][organ-1].append([list_list_x_feat[b] for list_x_feat in list_list_x_feat])\n",
    "                individual_data[key][\"xs\"][organ-1].append([list_xs[b] for list_xs in list_list_xs])\n",
    "                \n",
    "                # individual_data[key][\"biases\"][organ-1].append([list_biases[b] for list_biases in list_list_biases])\n",
    "                individual_data[key][\"weights\"][organ-1].append([list_weights[:,organ-1,:,:,:] for list_weights in list_list_weights[b]])\n",
    "\n",
    "\n",
    "\n",
    "                # individual_data[\"weights\"][organ-1].append(list_weights[b])\n",
    "\n",
    "\n",
    "    \n",
    "    ave_organ_dice = np.zeros((2, NUM_CLASS))\n",
    "    if args.local_rank == 0:\n",
    "        with open('out/'+args.log_name+f'/val_{args.epoch}.txt', 'w') as f:\n",
    "            for key in TEMPLATE.keys():\n",
    "                organ_list = TEMPLATE[key]\n",
    "                content = 'Task%s| '%(key)\n",
    "                for organ in organ_list:\n",
    "                    dice = dice_list[key][0][organ-1] / dice_list[key][1][organ-1]\n",
    "                    content += '%s: %.4f, '%(ORGAN_NAME[organ-1], dice)\n",
    "                    ave_organ_dice[0][organ-1] += dice_list[key][0][organ-1]\n",
    "                    ave_organ_dice[1][organ-1] += dice_list[key][1][organ-1]\n",
    "                print(content)\n",
    "                f.write(content)\n",
    "                f.write('\\n')\n",
    "            content = 'Average | '\n",
    "            for i in range(NUM_CLASS):\n",
    "                content += '%s: %.4f, '%(ORGAN_NAME[i], ave_organ_dice[0][organ-1] / ave_organ_dice[1][organ-1])\n",
    "            print(content)\n",
    "            f.write(content)\n",
    "            f.write('\\n')\n",
    "def sliding_window_inference_with_meta_data(image, roi_size, sw_batch_size, model):\n",
    "    list_x_feat, list_xs, list_weights, list_biases = [], [], [], []\n",
    "    def wrap_predictor(input_data):\n",
    "        ouput, x_feat, xs, weights,biases = model(input_data, True)\n",
    "        \n",
    "        list_x_feat.append(x_feat)\n",
    "        list_xs.append(xs)\n",
    "        list_weights.append(weights)\n",
    "        list_biases.append(biases)\n",
    "        return ouput\n",
    "    total_output = sliding_window_inference(image, roi_size, sw_batch_size, wrap_predictor)\n",
    "    return total_output, list_x_feat, list_xs, list_weights, list_biases\n",
    "    \n",
    "\n",
    "\n",
    "def valid_loop(data_root_path, dataset_list, datasetkey, num_workers, num_samples, cache_dataset, cache_rate, batch_size, roi, space,device=0):\n",
    "    class _object:\n",
    "        pass\n",
    "    \n",
    "    args = _object()\n",
    "    args.data_root_path = data_root_path\n",
    "    args.dataset_list = dataset_list\n",
    "    args.datasetkey = datasetkey\n",
    "    args.num_workers = num_workers\n",
    "    args.num_samples = num_samples\n",
    "    args.cache_dataset = cache_dataset\n",
    "    args.cache_rate = cache_rate\n",
    "    args.uniform_sample = True\n",
    "    args.batch_size = batch_size\n",
    "    args.roi_x = args.roi_y = args.roi_z = roi\n",
    "    args.space_x = args.space_y = args.space_z = space\n",
    "    args.a_min, args.a_max, args.b_min, args.b_max = -175,250,0.0,1.0\n",
    "    args.data_txt_path = './dataset/dataset_list/'\n",
    "    args.phase = \"validation\"\n",
    "    args.encoding = \"word_embedding\"\n",
    "    args.dist = False\n",
    "    args.lr = 1e-4\n",
    "    args.weight_decay = 1e-5\n",
    "    args.device = torch.device(f\"cuda:{device}\")\n",
    "    args.backbone = \"unet\"\n",
    "    args.max_epoch = 50\n",
    "    args.warmup_epoch = 5\n",
    "    args.log_name = \"unet\"\n",
    "    args.result_path = \"/data/result\"\n",
    "    args.word_embedding = \"./pretrained_weights/txt_encoding.pth\"\n",
    "    args.model_checkpoint = \"output/unet/epoch_50.pth\"\n",
    "    args.swap_word_embedding = 0\n",
    "    args.num_workers = 0\n",
    "    args.dist = False\n",
    "    args.device = 0\n",
    "    model = create_model(args.backbone, NUM_CLASS,trans_encoding=args.encoding, epoch=50, log_name=args.log_name, result_path=args.result_path, word_embedding=args.word_embedding, model_pretrain=None, model_checkpoint=args.model_checkpoint, roi_x=args.roi_x, roi_y=args.roi_y, roi_z=args.roi_z, swap_word_embedding=args.swap_word_embedding,device = args.device)\n",
    "    loss_seg_DICE = loss.DiceLoss(num_classes=NUM_CLASS).to(args.device)\n",
    "    loss_seg_CE = loss.Multi_BCELoss(num_classes=NUM_CLASS).to(args.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=args.warmup_epoch, max_epochs=args.max_epoch)\n",
    "    \n",
    "    val_loader, val_transform = get_loader(args)\n",
    "    if args.phase == \"train\":\n",
    "        train(args, val_loader, model, optimizer, loss_seg_DICE, loss_seg_CE)\n",
    "    else:\n",
    "        validation(model, val_loader, args)\n",
    "    # validation(model, val_loader, args)\n",
    "\n",
    "valid_loop('/data/clip/ClipReady/', ['PAOT_TEMP'], '01 04 05 07 08 09 10_03 10_06 10_07 10_08 10_09 10_10'.split(), 10, 2, False, 0.3, 1, 96, 96, 0)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 148)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'/data/clip/ClipReady/01_Multi-Atlas_Labeling/img/img0010.nii.gz'\n",
    "# load the file and checksize\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "img = nib.load('/data/clip/ClipReady/01_Multi-Atlas_Labeling/img/img0010.nii.gz')\n",
    "img_data = img.get_fdata()\n",
    "img_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01',\n",
       " '04',\n",
       " '05',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10_03',\n",
       " '10_06',\n",
       " '10_07',\n",
       " '10_08',\n",
       " '10_09',\n",
       " '10_10']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'01 04 05 07 08 09 10_03 10_06 10_07 10_08 10_09 10_10'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/dataset_list/P_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/dataset_list/P_train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f)\n",
      "File \u001b[0;32m/CLIP-Driven-Universal-Model/universal/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/dataset_list/P_train.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"./dataset/dataset_list/P_train.txt\",\"r\") as f:\n",
    "    print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
